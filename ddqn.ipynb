{"cells":[{"cell_type":"markdown","metadata":{"id":"rIB8h6ctx8pF"},"source":["# Code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50881,"status":"ok","timestamp":1705540034443,"user":{"displayName":"Rodrigo Pedro","userId":"01604731581156778290"},"user_tz":0},"id":"7u506Nbad1SA","outputId":"40b3a9e7-1497-4fb0-f496-d18d9ce4585f"},"outputs":[],"source":["%pip install highway-env==1.8.2 matplotlib==3.8.3 moviepy==1.0.3 numpy==1.26.4 pyvirtualdisplay==3.0 statsmodels==0.14.1 tensorflow==2.15.0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8904,"status":"ok","timestamp":1705540055854,"user":{"displayName":"Rodrigo Pedro","userId":"01604731581156778290"},"user_tz":0},"id":"KnEtZaIQeRDA"},"outputs":[],"source":["import random\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, Dropout, Dense, LSTM\n","\n","class NeuralNet():\n","    def __init__(self, input_dim, output_dim, learning_rate):\n","        # 2d conv net. Used for stack size = 1\n","        # Len 3 because the input_dim is (1, width, height)\n","        if len(input_dim) == 3:\n","            self.model = keras.Sequential([\n","                Conv2D(32, (3, 3), activation='relu', input_shape=input_dim, data_format='channels_first', padding='same'),\n","                MaxPooling2D((2, 2), strides=(2, 2)),\n","                Conv2D(64, (3, 3), activation='relu', data_format='channels_first', padding='same'),\n","                MaxPooling2D((2, 2), strides=(2, 2)),\n","                Flatten(),\n","                Dropout(0.5),\n","                Dense(128, activation='relu'),\n","                Dense(output_dim, activation=None)\n","            ])\n","\n","        # Time distributed 2d conv net. Use this for stack size > 1\n","        # Input shape becomes (stack_size, width, height, channels)\n","        # We need to add a channel dimension to the input shape, even if it's grayscale (because Conv2D expects a channel dimension).\n","        # TimeDistributed applies the conv layers to each image, instead of the whole stack. The weights are the same for each image.\n","        # Extra LSTM layer to process the time dimension\n","        # Will need a lot more training data to get good results\n","        else: # len(input_dim) == 4 if stack size > 1, where input_dim is (stack_size, width, height, 1)\n","            self.model = keras.Sequential([\n","                TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=input_dim),\n","                TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))),\n","                TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n","                TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))),\n","                TimeDistributed(Flatten()),\n","                LSTM(128, activation='relu', return_sequences=False),\n","                Dropout(0.5),\n","                Dense(128, activation='relu'),\n","                Dense(output_dim, activation=None)\n","            ])\n","\n","        self.model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n","\n","    def save_model(self, dir):\n","        self.model.save(dir)\n","\n","\n","    def load_model(self, dir):\n","        self.model = load_model(dir)\n","\n","\n","class Memory():\n","    def __init__(self, max_experiences, input_dim) -> None:\n","        self.states = np.zeros((max_experiences, *input_dim))\n","        self.actions = np.zeros(max_experiences, dtype=np.int32)\n","        self.new_states = np.zeros((max_experiences, *input_dim), dtype=np.float32)\n","        self.rewards = np.zeros(max_experiences, dtype=np.float32)\n","        self.dones = np.zeros(max_experiences, dtype=np.int32)\n","        self.exp_counter = 0 # num experiences. Always incrementing and never reset\n","        self.index = 0 # index of the last experience added. Looping from 0 to max_experiences\n","        self.max_experiences = max_experiences # max experiences in memory\n","\n","\n","    def add_experience(self, state, action, new_state, reward, done):\n","        self.index = self.exp_counter % self.max_experiences\n","\n","        self.states[self.index] = state\n","        self.actions[self.index] = action\n","        self.new_states[self.index] = new_state\n","        self.rewards[self.index] = reward\n","        self.dones[self.index] = done\n","\n","        self.exp_counter += 1\n","        self.index += 1\n","\n","    def get_experience_batch(self, batch_size):\n","        indices = np.random.choice(min(self.exp_counter, self.max_experiences), batch_size, replace=False)\n","\n","        out_states = self.states[indices]\n","        out_actions = self.actions[indices]\n","        out_new_states = self.new_states[indices]\n","        out_rewards = self.rewards[indices]\n","        out_dones = self.dones[indices]\n","\n","        return out_states, out_actions, out_new_states, out_rewards, out_dones\n","\n","\n","class Agent():\n","    def __init__(self, max_experiences, input_dim, num_actions, learning_rate, batch_size, exploration_rate_start, gamma, exploration_rate_decrement, exploration_rate_min, update_target_iters) -> None:\n","        self.memory = Memory(max_experiences, input_dim)\n","        self.batch_size = batch_size\n","        self.exploration_rate = exploration_rate_start\n","        self.num_actions = num_actions\n","        self.gamma = gamma\n","        self.exploration_rate_decrement = exploration_rate_decrement\n","        self.exploration_rate_min = exploration_rate_min\n","        self.update_target_iters = update_target_iters\n","\n","        # estimate best action\n","        self.neural_net = NeuralNet(input_dim, num_actions, learning_rate)\n","        # target - estimate q val of selected action\n","        self.target_neural_net = NeuralNet(input_dim, num_actions, learning_rate)\n","\n","        # copy weights from neural_net to target_neural_net\n","        self.target_neural_net.model.set_weights(self.neural_net.model.get_weights())\n","\n","    # 1ยบ - choose action (choose_action)\n","    # 2ยบ - add experience to memory (memory.add)\n","    # 3ยบ - choose batch, learn (learn)\n","\n","    def decrement_exploration_rate(self):\n","        self.exploration_rate = max(self.exploration_rate - self.exploration_rate_decrement, self.exploration_rate_min)\n","\n","    def choose_action(self, state):\n","        output = None\n","\n","        if self.exploration_rate > random.random():   # explore\n","            output = random.randrange(self.num_actions)\n","        else:                                         # choose best action\n","            output = np.argmax(self.neural_net.model.predict(np.array([state]), verbose=None))\n","\n","        return output\n","\n","    def learn(self,):\n","        # can't fill the batch yet, do nothing\n","        if self.memory.exp_counter < self.batch_size:\n","            return\n","\n","        batch_states, batch_actions, batch_new_states, batch_rewards, batch_dones = self.memory.get_experience_batch(self.batch_size)\n","\n","        print(batch_states.shape)\n","        print(batch_new_states.shape)\n","\n","        # target network q-values on new state\n","        target_new_state_qs = self.target_neural_net.model.predict_on_batch(batch_new_states)\n","\n","        # online network q-value of new state\n","        new_state_qs = self.neural_net.model.predict_on_batch(batch_new_states)\n","\n","        # get best action on new state using online model\n","        # argmax returns the index of the max value in the array\n","        argmax_action = np.argmax(new_state_qs, axis=1)\n","\n","        # q-values of current state. Will be used as the target y after updating the q-values\n","        target_qs = self.neural_net.model.predict_on_batch(batch_states)\n","        # need to create this array to access the correct batch in the next line\n","        batch_index = np.arange(self.batch_size)\n","        # update q-values of actions taken\n","        target_qs[batch_index, batch_actions] = batch_rewards + (1-batch_dones) * self.gamma * target_new_state_qs[batch_index, argmax_action]\n","\n","        # target_qs will be the target. The method runs forward pass and calculates back prop, therefore we provide batch_states as input\n","        self.neural_net.model.fit(batch_states, target_qs, verbose=0)\n","\n","        self.decrement_exploration_rate()\n","\n","        # update target network every update_target_iters iterations\n","        if self.memory.exp_counter % self.update_target_iters == 0:\n","            self.target_neural_net.model.set_weights(self.neural_net.model.get_weights())"]},{"cell_type":"markdown","metadata":{"id":"RzTYKWE9xrjH"},"source":["# Setup environment\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["width = 128\n","height = 64\n","stack_size = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":695,"status":"ok","timestamp":1705540060770,"user":{"displayName":"Rodrigo Pedro","userId":"01604731581156778290"},"user_tz":0},"id":"6aeqvnldxqNI","outputId":"59006be3-8da0-4d18-86be-befbd514f59a"},"outputs":[],"source":["import gymnasium as gym\n","from gymnasium.wrappers import TransformObservation\n","\n","# choose between highway-fast-v0, merge-v0, roundabout-v0, intersection-v0\n","env = gym.make('highway-fast-v0', render_mode=\"rgb_array\")\n","\n","env.unwrapped.configure({\n","    \"normalize_reward\": True,\n","\n","    \"observation\": {\n","        \"type\": \"GrayscaleObservation\",\n","        \"observation_shape\": (width, height),\n","        \"stack_size\": stack_size,\n","        \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n","        \"scaling\": 1.75,\n","    },\n","})\n","\n","# add channel dimension to input shape if stack_size > 1\n","input_dim = (stack_size, width, height, 1) if stack_size > 1 else (stack_size, width, height)\n","if stack_size > 1:\n","    env = TransformObservation(env, lambda obs: np.expand_dims(obs, axis=-1))"]},{"cell_type":"markdown","metadata":{"id":"nZHZgU4ldv8S"},"source":["# Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1354882,"status":"ok","timestamp":1705541420670,"user":{"displayName":"Rodrigo Pedro","userId":"01604731581156778290"},"user_tz":0},"id":"URyXP7JneY25","outputId":"5f22d1c0-54e6-4010-a1e2-37b5ae25d932"},"outputs":[],"source":["episodes = 500\n","\n","agent = Agent(\n","    max_experiences=15000,\n","    input_dim=input_dim,\n","    num_actions=env.action_space.n,\n","    learning_rate=5e-4,\n","    batch_size=32,\n","    exploration_rate_start=1.0,\n","    gamma=0.99,\n","    exploration_rate_decrement=0.005,\n","    exploration_rate_min=0.001,\n","    update_target_iters=50,\n",")\n","\n","scores = []\n","\n","for i in range(episodes):\n","    obs, info = env.reset()\n","    current_score = 0\n","\n","    done = truncated = False\n","    while not (done or truncated):\n","        action = agent.choose_action(obs)\n","\n","        new_obs, reward, done, truncated, info = env.step(action)\n","\n","        agent.memory.add_experience(obs, action, new_obs, reward, done)\n","        agent.learn()\n","        # env.render()\n","        obs = new_obs\n","\n","        current_score += reward\n","\n","    scores.append(current_score)\n","\n","    print(\"Episode: {}/{} Score: {}\".format(i + 1, episodes, current_score))\n","\n","print(\"Average score: {}\".format(np.average(scores)))\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"2nwJ3Orax8_1"},"source":["# Training graphs"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":699,"status":"ok","timestamp":1705541427151,"user":{"displayName":"Rodrigo Pedro","userId":"01604731581156778290"},"user_tz":0},"id":"052ATFA88UJZ"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import statsmodels.api as sm\n","\n","def graph(x,y):\n","  # Calculate Lowess smoothed line\n","  lowess = sm.nonparametric.lowess(y, x, frac=0.3)\n","\n","  # Calculate residuals and estimate uncertainty bounds\n","  residuals = y - lowess[:, 1]\n","  std_dev = np.std(residuals)\n","  lower_bound = lowess[:, 1] - 2 * std_dev\n","  upper_bound = lowess[:, 1] + 2 * std_dev\n","\n","  # Plotting\n","  plt.figure(figsize=(10, 6))\n","  plt.plot(lowess[:, 0], lowess[:, 1], label='Lowess smoothed line', color='blue')\n","  plt.fill_between(lowess[:, 0], lower_bound, upper_bound, color='blue', alpha=0.3, label='Uncertainty bounds')\n","  plt.scatter(x, y, alpha=0.5, label='Data points', color='grey')\n","  plt.xlabel('Episodes')\n","  plt.ylabel('Scores')\n","  plt.title('Scores over episodes')\n","  plt.legend()\n","  plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"kktej1fXuulg"},"source":["Load list of scores, display them in graph"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"executionInfo":{"elapsed":2945,"status":"ok","timestamp":1705541513589,"user":{"displayName":"Rodrigo Pedro","userId":"01604731581156778290"},"user_tz":0},"id":"8CToU2uR5Bjn","outputId":"6e0c51fc-12c6-4567-907d-72988d5de9af"},"outputs":[],"source":["x = np.arange(start=1, stop=episodes+1)\n","y = np.array(scores)\n","\n","graph(np.arange(start=1, stop=episodes+1), np.array(scores))"]},{"cell_type":"markdown","metadata":{"id":"-rGl1nXKFL3X"},"source":["# Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23408,"status":"ok","timestamp":1705541547809,"user":{"displayName":"Rodrigo Pedro","userId":"01604731581156778290"},"user_tz":0},"id":"vOfp2EQ1FN-x","outputId":"b22051cf-f0df-4007-8be3-7de1cb9292c7"},"outputs":[],"source":["agent.neural_net.save_model(\"model-ddqn\")\n","agent.target_neural_net.save_model(\"model-ddqn-target\")"]},{"cell_type":"markdown","metadata":{"id":"xgT3-P_sc09y"},"source":["# Test existing model\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualization utils. https://github.com/Farama-Foundation/HighwayEnv/blob/master/scripts/utils.py\n","import base64\n","from pathlib import Path\n","from gymnasium.wrappers import RecordVideo\n","from IPython import display as ipythondisplay\n","\n","def record_videos(env, video_folder=\"videos\"):\n","    wrapped = RecordVideo(\n","        env, video_folder=video_folder, episode_trigger=lambda e: True\n","    )\n","\n","    # Capture intermediate frames\n","    env.unwrapped.set_record_video_wrapper(wrapped)\n","\n","    return wrapped\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9557,"status":"ok","timestamp":1705541565910,"user":{"displayName":"Rodrigo Pedro","userId":"01604731581156778290"},"user_tz":0},"id":"VlsgZewEc7_c","outputId":"1268384e-434d-49e9-f961-913882858508"},"outputs":[],"source":["import datetime\n","\n","now = datetime.datetime.now()\n","now_str = now.strftime(\"%Y%m%d_%H%M%S\")\n","video_folder = f\"videos_{now_str}\"\n","\n","_env = record_videos(env, video_folder=video_folder)\n","\n","episodes = 5\n","\n","agent = Agent(\n","    max_experiences=15000,\n","    input_dim=input_dim,\n","    num_actions=env.action_space.n,\n","    learning_rate=5e-4,\n","    batch_size=32,\n","    exploration_rate_start=0.0,\n","    gamma=0.99,\n","    exploration_rate_decrement=0,\n","    exploration_rate_min=0,\n","    update_target_iters=50,\n",")\n","\n","# load model file\n","agent.neural_net.load_model(\"model-ddqn\")\n","agent.target_neural_net.load_model(\"model-ddqn-target\")\n","\n","scores = []\n","crashes = []\n","speeds = []\n","\n","for i in range(episodes):\n","    obs, info = _env.reset()\n","\n","    current_score = 0\n","    current_ep_speeds = []\n","    hasCrashed = False\n","\n","    done = truncated = False\n","    while not (done or truncated):\n","        action = agent.choose_action(obs)\n","        new_obs, reward, done, truncated, info = _env.step(action)\n","\n","        if _env.unwrapped.vehicle.crashed:\n","            hasCrashed = True\n","\n","        current_ep_speeds.append(_env.unwrapped.vehicle.speed)\n","\n","        _env.render()\n","        obs = new_obs\n","\n","        current_score += reward\n","\n","\n","    average_speed = np.average(np.array(current_ep_speeds))\n","\n","    scores.append(current_score)\n","    crashes.append(hasCrashed)\n","    speeds.append(average_speed)\n","\n","    print(\n","        \"Episode: {}/{} Score: {} Crashed: {} Average speed: {}\".format(\n","            i + 1, episodes, current_score, hasCrashed, average_speed\n","        )\n","    )\n","\n","print(\"Average score: {}\".format(np.average(scores)))\n","\n","_env.close()\n","\n","# show all videos\n","video_path = Path(video_folder)\n","video_files = list(video_path.glob(\"*.mp4\"))\n","video_files.sort()\n","for video_file in video_files:\n","    print(video_file)\n","    video = open(video_file, \"rb\").read()\n","    video_url = f\"data:video/mp4;base64,{base64.b64encode(video).decode()}\"\n","    ipythondisplay.display(\n","        ipythondisplay.HTML(\n","            f\"\"\"<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n","        <source src=\"{video_url}\" type=\"video/mp4\" />\n","    </video>\"\"\"\n","        )\n","    )"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPJMlGFZq/AhIPUT4YejWrP","collapsed_sections":["RzTYKWE9xrjH"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
